# -*- coding: utf-8 -*-
"""TakeHomeTestDS_Chatharina Cheisha.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzjrYL14UJxsd2FptWBJTCzvRgBI1B0A

# Prediksi Waktu Pengantaran Makanan

# Tujuan:
Membangun model prediktif untuk memperkirakan Delivery_Time_min (menit) per pesanan pada layanan pengantaran makanan, agar tim operasional dapat mengestimasi ETA yang lebih akurat dan mengoptimalkan alokasi kurir.
"""

import matplotlib.pyplot as plt
import os, datetime as dt
import numpy as np
import pandas as pd
import streamlit as st
import altair as alt

# sklearn
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer
from sklearn.linear_model import Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance, PartialDependenceDisplay
from inspect import signature

st.set_page_config(page_title="Prediksi Waktu Pengantaran", layout="wide")
st.title("ðŸ“¦ Prediksi Waktu Pengantaran Makanan (ETA)")
st.caption("Versi Streamlit dengan tooltip interaktif")

"""Data Undertanding"""

@st.cache_data(show_spinner=True)
def load_data():
    file_id = '1qI18G7Rjr5Axqz-HawadpTzSnTwf5jeQ'
    url = f'https://drive.google.com/uc?export=download&id={file_id}'
    df = pd.read_csv(url)
    return df

df = load_data()

st.subheader("Data Understanding")
c1, c2, c3 = st.columns([2,2,3])
with c1:
    st.write("Shape:", df.shape)
    st.dataframe(df.head(), use_container_width=True)
with c2:
    st.write("Describe")
    st.dataframe(df.describe().T, use_container_width=True)
with c3:
    st.write("Dtypes")
    st.json({col: str(tp) for col, tp in df.dtypes.items()})

# data cleaning

cat_cols = df.select_dtypes(include=["object", "string"]).columns
for c in cat_cols:
    df[c] = (df[c].astype("string").str.strip().str.replace(r"\s+", " ", regex=True))

for c in cat_cols:
    m = df[c].mode(dropna=True)
    if not m.empty:
        df[c] = df[c].fillna(m.iat[0])

candidate_cols = ["Courier_Experience_yrs"]
col = next((c for c in candidate_cols if c in df.columns), None)
if col is None:
    st.error(f"Kolom tidak ditemukan. Kolom tersedia: {list(df.columns)}")
    st.stop()

df[col] = pd.to_numeric(df[col], errors="coerce")
df[col] = df[col].fillna(df[col].median(skipna=True))

TARGET = "Delivery_Time_min"
assert TARGET in df.columns, f"Target '{TARGET}' tidak ada."

id_like = [c for c in df.columns if c.lower() in {"order_id", "id"}]
X = df.drop(columns=[TARGET] + id_like, errors="ignore")
y = df[TARGET]

if "Courier_Experince_yrs" in X.columns and "Courier_Experience_yrs" not in X.columns:
    X = X.rename(columns={"Courier_Experince_yrs": "Courier_Experience_yrs"})

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
st.success(f"Split data â†’ X_train: {X_train.shape}, X_test: {X_test.shape}")

cat_cols = df.select_dtypes(include=["object", "string"]).columns

for c in cat_cols:
  df[c] = (df[c].astype("string")
    .str.strip()
    .str.replace(r"\s+", " ", regex=True))

for c in cat_cols:
  m = df[c].mode(dropna=True)
  if m.empty:
    raise ValueError(f"Kolom '{c}' semua NaN, tidak ada modus untuk dipakai.")
  df[c] = df[c].fillna(m.iat[0])

candidate_cols = ["Courier_Experience_yrs"]
col = next((c for c in candidate_cols if c in df.columns), None)
if col is None:
  raise KeyError(f"Kolom tidak ditemukan. Kolom tersedia: {list(df.columns)}")

df[col] = pd.to_numeric(df[col], errors="coerce")

median_val = df[col].median(skipna=True)
df[col] = df[col].fillna(median_val)

missing_percentage = df.isnull().sum()/df.shape[0]*100
missing_percentage.sort_values(ascending=False)

df.duplicated().sum()

"""Pembagian Data Set"""

TARGET = "Delivery_Time_min"
assert TARGET in df.columns, f"Target '{TARGET}' tidak ada."

# Deteksi kolom ID umum agar tidak jadi fitur
id_like = [c for c in df.columns if c.lower() in {"order_id", "id"}]
X = df.drop(columns=[TARGET] + id_like, errors="ignore")
y = df[TARGET]

# Perbaiki tipe kolom yang mungkin typo
if "Courier_Experince_yrs" in X.columns and "Courier_Experience_yrs" not in X.columns:
    X = X.rename(columns={"Courier_Experince_yrs": "Courier_Experience_yrs"})

# Split train/test (80/20)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print(f"X_train: {X_train.shape}, X_test: {X_test.shape}")

"""EDA ringan + importance eksploratif

EDA ringan di TRAIN
"""

num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X_train.select_dtypes(include=["object", "string"]).columns.tolist()

c1, c2 = st.columns(2)
with c1:
    miss = ((X_train.isna().mean() * 100).sort_values(ascending=False)).round(2)
    st.write("Missing % per kolom (TRAIN)")
    st.dataframe(miss, use_container_width=True)
with c2:
    st.write("Target stats (TRAIN)")
    st.dataframe(y_train.describe()[["count","mean","std","min","25%","50%","75%","max"]].round(2))

if len(num_cols) > 1:
    corr = X_train[num_cols].join(y_train).corr()[TARGET].drop(TARGET).sort_values(ascending=False)
    corr_df = corr.round(3).reset_index().rename(columns={'index': 'feature', TARGET: 'corr'})
    st.write("Korelasi numerik vs target (TRAIN)")
    st.altair_chart(
        alt.Chart(corr_df).mark_bar().encode(
            x=alt.X("corr:Q"),
            y=alt.Y("feature:N", sort='-x'),
            tooltip=["feature", "corr"]
        ).properties(height=max(200, 20*len(corr_df))),
        use_container_width=True
    )

""" Feature Imprtance Eksploratif"""

# Preprocess untuk importance (imputer & OHE)
num_transform = Pipeline([("imputer", SimpleImputer(strategy="median"))])
cat_transform = Pipeline([("imputer", SimpleImputer(strategy="most_frequent")),
                          ("ohe", OneHotEncoder(handle_unknown="ignore"))])

preprocess = ColumnTransformer(
    transformers=[("num", num_transform, num_cols),
                  ("cat", cat_transform, cat_cols)]
)

rf = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
pipe_rf = Pipeline([("preprocess", preprocess), ("model", rf)])

# Scorers
def _rmse(y_true, y_pred): return np.sqrt(mean_squared_error(y_true, y_pred))
mae_scorer  = make_scorer(mean_absolute_error, greater_is_better=False)
rmse_scorer = make_scorer(_rmse, greater_is_better=False)
r2_scorer   = make_scorer(r2_score)

scorers = {"MAE": mae_scorer, "RMSE": rmse_scorer, "R2": r2_scorer}

# Importance eksploratif (ringkas)
cv = KFold(n_splits=5, shuffle=True, random_state=42)
feat_orig = list(num_cols) + list(cat_cols)
imp_mae_folds, imp_rmse_folds, imp_r2_folds = [], [], []

for k, (tr_idx, va_idx) in enumerate(cv.split(X_train, y_train), 1):
    X_tr, X_va = X_train.iloc[tr_idx].copy(), X_train.iloc[va_idx].copy()
    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[va_idx]

    pipe_rf.fit(X_tr, y_tr)
    yhat_base = pipe_rf.predict(X_va)
    base_mae  = mean_absolute_error(y_va, yhat_base)
    base_rmse = np.sqrt(mean_squared_error(y_va, yhat_base))
    base_r2   = r2_score(y_va, yhat_base)

    rng = np.random.default_rng(42 + k)
    d_mae, d_rmse, d_r2 = {}, {}, {}
    for f in feat_orig:
        Xp = X_va.copy()
        v = Xp[f].to_numpy(copy=True); rng.shuffle(v); Xp[f] = v
        yhat_p = pipe_rf.predict(Xp)
        d_mae[f]  = mean_absolute_error(y_va, yhat_p) - base_mae
        d_rmse[f] = np.sqrt(mean_squared_error(y_va, yhat_p)) - base_rmse
        d_r2[f]   = base_r2 - r2_score(y_va, yhat_p)
    imp_mae_folds.append(d_mae); imp_rmse_folds.append(d_rmse); imp_r2_folds.append(d_r2)

def _avg(tbls):
    keys = set().union(*[t.keys() for t in tbls])
    return (pd.Series({k: np.mean([t.get(k, 0.0) for t in tbls]) for k in keys})
            .sort_values(ascending=False).reset_index())

imp_mae_tbl  = _avg(imp_mae_folds);  imp_mae_tbl.columns  = ["feature","perm_importance_MAE"]

st.subheader("Exploratory Permutation Importance (Î”MAE)")
st.altair_chart(
    alt.Chart(imp_mae_tbl.head(20)).mark_bar().encode(
        x=alt.X("perm_importance_MAE:Q", title="Î”MAE (lebih besar = lebih penting)"),
        y=alt.Y("feature:N", sort='-x'),
        tooltip=["feature", alt.Tooltip("perm_importance_MAE:Q", format=".4f")]
    ).properties(height=400),
    use_container_width=True
)

"""Tuning via CV"""

def rmse_metric(y_true, y_pred):
    try: return mean_squared_error(y_true, y_pred, squared=False)
    except TypeError: return np.sqrt(mean_squared_error(y_true, y_pred))

preprocess_tree = ColumnTransformer([
    ("num", Pipeline([("imp", SimpleImputer(strategy="median"))]), num_cols),
    ("cat", Pipeline([("imp", SimpleImputer(strategy="most_frequent")),
                      ("ohe", OneHotEncoder(handle_unknown="ignore"))]), cat_cols),
])
preprocess_linear = ColumnTransformer([
    ("num", Pipeline([("imp", SimpleImputer(strategy="median")), ("sc", StandardScaler())]), num_cols),
    ("cat", Pipeline([("imp", SimpleImputer(strategy="most_frequent")),
                      ("ohe", OneHotEncoder(handle_unknown="ignore"))]), cat_cols),
])

cv = KFold(n_splits=5, shuffle=True, random_state=42)
scoring = "neg_mean_absolute_error"

cands = [
  ("Ridge",
   Pipeline([("preprocess", preprocess_linear), ("model", Ridge())]),
   {"model__alpha":[0.1, 1.0, 10.0]}),

  ("Lasso",
   Pipeline([("preprocess", preprocess_linear), ("model", Lasso(max_iter=10000))]),
   {"model__alpha":[0.001, 0.01, 0.1, 1.0]}),

  ("Decision Tree",
   Pipeline([("preprocess", preprocess_tree), ("model", DecisionTreeRegressor(random_state=42))]),
   {"model__max_depth":[3,5,8,None], "model__min_samples_leaf":[1,3,5]}),

  ("Random Forest",
   Pipeline([("preprocess", preprocess_tree), ("model", RandomForestRegressor(random_state=42, n_jobs=-1))]),
   {"model__n_estimators":[200,400], "model__max_depth":[None,8,12], "model__min_samples_leaf":[1,2,4]}),
]

has_xgb = False
try:
    from xgboost import XGBRegressor
    has_xgb = True
    cands.append((
        "XGBoost",
        Pipeline([("preprocess", preprocess_tree),
                  ("model", XGBRegressor(random_state=42, n_jobs=-1, tree_method="hist", eval_metric="mae"))]),
        {"model__n_estimators":[300,600], "model__max_depth":[3,6],
         "model__learning_rate":[0.05,0.1], "model__subsample":[0.8,1.0], "model__colsample_bytree":[0.8,1.0]}
    ))
except Exception:
    pass

rows, best_models = [], {}
for name, pipe, grid in cands:
    pipe.fit(X_train, y_train)
    y_hat_pre = pipe.predict(X_test)
    mae_pre  = mean_absolute_error(y_test, y_hat_pre)
    rmse_pre = rmse_metric(y_test, y_hat_pre)
    r2_pre   = r2_score(y_test, y_hat_pre)

    gs = GridSearchCV(pipe, grid, cv=cv, scoring=scoring, n_jobs=-1, refit=True, verbose=0)
    gs.fit(X_train, y_train)
    best_models[name] = gs.best_estimator_

    y_hat_post = gs.predict(X_test)
    rows.append({
        "model": name,
        "mae_pre": mae_pre, "rmse_pre": rmse_pre, "r2_pre": r2_pre,
        "mae_post": mean_absolute_error(y_test, y_hat_post),
        "rmse_post": rmse_metric(y_test, y_hat_post),
        "r2_post": r2_score(y_test, y_hat_post),
        "best_params": gs.best_params_,
    })

cmp = pd.DataFrame(rows).sort_values("mae_post").reset_index(drop=True)
st.subheader("Perbandingan Model (Before vs After Tuning)")
st.dataframe(cmp, use_container_width=True)

best_name = cmp.loc[0, "model"]
best_pipe = best_models[best_name]
st.success(f"Best by CV (MAE): **{best_name}**")
pipe = best_pipe

"""Pemilihan Model"""

## Preprocessor
num_transform_tree = Pipeline(steps=[
  ("imputer", SimpleImputer(strategy="median"))
])
cat_transform = Pipeline(steps=[
  ("imputer", SimpleImputer(strategy="most_frequent")),
  ("ohe", OneHotEncoder(handle_unknown="ignore"))
])

num_transform_linear = Pipeline(steps=[
  ("imputer", SimpleImputer(strategy="median")),
  ("scaler", StandardScaler())
])

# ColumnTransformer untuk tree-models dan linear-models
preprocess_tree = ColumnTransformer(
  transformers=[
    ("num", num_transform_tree, num_cols),
    ("cat", cat_transform, cat_cols),
  ],
  remainder="drop"
)

preprocess_linear = ColumnTransformer(
  transformers=[
    ("num", num_transform_linear, num_cols),
    ("cat", cat_transform, cat_cols),
  ],
  remainder="drop"
)

""" Linear Regression: Ridge & Lasso"""

# Helper: inisiasi estimator hanya dengan parameter yang didukung
def init_supported(estimator_cls, **kwargs):
  """Return estimator_cls(**filtered_kwargs) where unsupported keys are dropped."""
  params = signature(estimator_cls.__init__).parameters
  filtered = {k: v for k, v in kwargs.items() if k in params}
  return estimator_cls(**filtered)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

def evaluate(model, X_tr, y_tr, X_te, y_te, name="Model"):
  pred_tr = model.predict(X_tr)
  pred_te = model.predict(X_te)
  mae_tr = mean_absolute_error(y_tr, pred_tr)
  mae_te = mean_absolute_error(y_te, pred_te)
  rmse_te = np.sqrt(mean_squared_error(y_te, pred_te))
  r2_te = r2_score(y_te, pred_te)

  st.write(f"\n {name}")
  st.write(f"Train MAE : {mae_tr:.3f}")
  st.write(f"Test  MAE : {mae_te:.3f}")
  st.write(f"Test  RMSE: {rmse_te:.3f}")
  st.write(f"Test  R2  : {r2_te:.3f}")
  return {"name": name, "mae": mae_te, "rmse": rmse_te, "r2": r2_te}

results = []
ridge_pipe = Pipeline(steps=[
  ("preprocess", preprocess_linear),
  ("model", init_supported(Ridge, alpha=1.0, random_state=42))
])
ridge_pipe.fit(X_train, y_train)
res_ridge = evaluate(ridge_pipe, X_train, y_train, X_test, y_test, "Ridge")
results.append(res_ridge)

lasso_pipe = Pipeline(steps=[
    ("preprocess", preprocess_linear),
    ("model", init_supported(Lasso, alpha=0.001, random_state=42, max_iter=10000))
])
lasso_pipe.fit(X_train, y_train)
res_lasso = evaluate(lasso_pipe, X_train, y_train, X_test, y_test, "Lasso")
results.append(res_lasso)

## Decision Tree"""

tree_pipe = Pipeline(steps=[
  ("preprocess", preprocess_tree),
  ("model", init_supported(DecisionTreeRegressor, random_state=42, max_depth=None))
])
tree_pipe.fit(X_train, y_train)
res_tree = evaluate(tree_pipe, X_train, y_train, X_test, y_test, "Decision Tree")
results.append(res_tree)

## Random Forest"""

rf_pipe = Pipeline(steps=[
  ("preprocess", preprocess_tree),
  ("model", init_supported(RandomForestRegressor, n_estimators=300, random_state=42, n_jobs=-1))
])
rf_pipe.fit(X_train, y_train)
res_rf = evaluate(rf_pipe, X_train, y_train, X_test, y_test, "Random Forest")
results.append(res_rf)

## XGBoost"""

if has_xgb:
  xgb_pipe = Pipeline(steps=[
    ("preprocess", preprocess_tree),
    ("model", init_supported(XGBRegressor,
      n_estimators=500, learning_rate=0.05, max_depth=6,
      subsample=0.8, colsample_bytree=0.8,
      reg_lambda=1.0, random_state=42, n_jobs=-1,
      tree_method="hist"))
  ])
  xgb_pipe.fit(X_train, y_train)
  res_xgb = evaluate(xgb_pipe, X_train, y_train, X_test, y_test, "XGBoost")
  results.append(res_xgb)

## Tabel Ringkasan (Evaluasi)"""

import pandas as pd

# Kumpulkan hasil yang ADA saja (abaikan yang belum dibuat)
cands = ['res_ridge', 'res_lasso', 'res_tree', 'res_rf', 'res_xgb']
results = []
for k in cands:
  if k in globals() and isinstance(globals()[k], dict):
    d = globals()[k]
    if all(m in d for m in ('name','mae','rmse','r2')):
      results.append(d)

# Jadiin tabel, urut MAE, kasih rank, bulatkan tampilan
df_res = pd.DataFrame(results, columns=['name','mae','rmse','r2'])
df_res = df_res.sort_values('mae').reset_index(drop=True)
df_res.insert(0, 'rank', range(1, len(df_res)+1))
df_res[['mae','rmse','r2']] = df_res[['mae','rmse','r2']].round(3)

st.dataframe(df_res, use_container_width=True)

"""Feature Importance untuk model TERBAIK"""
pipe = best_pipe
ct  = pipe.named_steps['preprocess']
try:
    feat_names = list(ct.get_feature_names_out())
except Exception:
    num_names = list(ct.transformers_[0][2])
    cat_base  = ct.transformers_[1][2]
    ohe       = ct.named_transformers_['cat'].named_steps['ohe']
    feat_names = num_names + list(ohe.get_feature_names_out(cat_base))

mdl = pipe.named_steps['model']
fi_intrinsic = None
if hasattr(mdl, "feature_importances_"):
    fi_intrinsic = pd.DataFrame({"feature": feat_names, "importance": mdl.feature_importances_}) \
                    .sort_values("importance", ascending=False)
elif hasattr(mdl, "coef_"):
    coef = np.ravel(mdl.coef_)
    fi_intrinsic = pd.DataFrame({"feature": feat_names, "importance": np.abs(coef)}) \
                    .sort_values("importance", ascending=False)

st.subheader("Intrinsic Importance / Coefficients")
if fi_intrinsic is not None:
    st.altair_chart(
        alt.Chart(fi_intrinsic.head(20)).mark_bar().encode(
            x=alt.X("importance:Q"),
            y=alt.Y("feature:N", sort='-x'),
            tooltip=["feature", alt.Tooltip("importance:Q", format=".4f")]
        ).properties(height=400),
        use_container_width=True
    )
else:
    st.info("Model tidak expose importance/coef.")

st.subheader("Permutation Importance (model-agnostic, Î”MAE)")
r = permutation_importance(
    pipe, X_test, y_test,
    n_repeats=20, random_state=42, n_jobs=-1,
    scoring=lambda est, X, y: -mean_absolute_error(y, est.predict(X))
)
imp = -r.importances_mean
k = min(len(feat_names), len(imp))
perm_mae = (pd.DataFrame({"feature": np.array(feat_names)[:k], "perm_importance_MAE": imp[:k]})
            .sort_values("perm_importance_MAE", ascending=False).head(20))
st.altair_chart(
    alt.Chart(perm_mae).mark_bar().encode(
        x=alt.X("perm_importance_MAE:Q", title="Î”MAE"),
        y=alt.Y("feature:N", sort='-x'),
        tooltip=["feature", alt.Tooltip("perm_importance_MAE:Q", format=".4f")]
    ).properties(height=400),
    use_container_width=True
)

"""Visualisasi

Parity + Residuals
"""

y_pred = pipe.predict(X_test)
resid  = y_test - y_pred

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1) Parity plot
axes[0].scatter(y_test, y_pred, alpha=0.6)
lo, hi = float(min(y_test.min(), y_pred.min())), float(max(y_test.max(), y_pred.max()))
axes[0].plot([lo, hi], [lo, hi], '--')
axes[0].set_xlabel("Actual"); axes[0].set_ylabel("Predicted"); axes[0].set_title("Actual vs Predicted")

# 2) Histogram residual
axes[1].hist(resid, bins=30)
axes[1].set_title("Residual Histogram")
axes[1].set_xlabel("Residual (Actual - Pred)"); axes[1].set_ylabel("Count")

# 3) Residual vs Predicted
axes[2].scatter(y_pred, resid, alpha=0.6)
axes[2].axhline(0, linestyle='--')
axes[2].set_xlabel("Predicted"); axes[2].set_ylabel("Residual"); axes[2].set_title("Residual vs Predicted")

plt.tight_layout()
st.pyplot(plt.gcf())

"""Error per segmen (opsional cepat)"""

df_te = X_test.copy()
df_te["y_true"] = y_test
df_te["y_pred"] = y_pred
df_te["abs_err"] = (df_te["y_true"] - df_te["y_pred"]).abs()

for c in [col for col in ["Weather","Traffic_Level","Time_of_Day","Vehicle_Type"] if col in df_te.columns]:
    seg = (df_te.groupby(c)["abs_err"].agg(["count","mean","median","max"]).sort_values("mean"))
    st.markdown(f"**Segment MAE by `{c}`**")
    st.dataframe(seg.head(10), use_container_width=True)

"""PDP (Partial Dependence) untuk fitur numerik utama"""
# Pilih kandidat fitur
cand = [f for f in ["Distance_km","Preparation_Time_min","Courier_Experience_yrs"] if f in X_test.columns]
if not cand:
    cand = num_cols[:min(3, len(num_cols))]

# PDP kind="average"
fig, axes = plt.subplots(1, len(cand), figsize=(6*len(cand), 4))
if len(cand) == 1:
    axes = [axes]
for ax, f in zip(axes, cand):
    PartialDependenceDisplay.from_estimator(
        pipe, X_test, [f],
        kind="average",
        grid_resolution=50,
        ax=ax
    )
    ax.set_title(f"PDP â€” {f}")
plt.tight_layout()
st.pyplot(fig)

# PDP kind="both"
fig, axes = plt.subplots(1, len(cand), figsize=(6*len(cand), 4))
if len(cand) == 1:
    axes = [axes]
for ax, f in zip(axes, cand):
    PartialDependenceDisplay.from_estimator(
        pipe, X_test, [f],
        kind="both",
        grid_resolution=50,
        ax=ax
    )
    ax.set_title(f"PDP â€” {f}")
plt.tight_layout()
st.pyplot(fig)

# PDP pairs
pairs = [("Distance_km","Preparation_Time_min"),
         ("Distance_km","Courier_Experience_yrs")]
for a, b in pairs:
    if a in X_test.columns and b in X_test.columns:
        fig, ax = plt.subplots(1, 1, figsize=(6, 4))
        PartialDependenceDisplay.from_estimator(pipe, X_test, [(a, b)], grid_resolution=30, ax=ax)
        ax.set_title(f"PDP â€” ({a}, {b})")
        st.pyplot(fig)
        
for name, p in [("Random Forest", rf_pipe if 'rf_pipe' in globals() else None),
                ("XGBoost", xgb_pipe if 'xgb_pipe' in globals() else None)]:
    if p is not None:
        for f in ["Distance_km","Preparation_Time_min","Courier_Experience_yrs"]:
            if f in X_test.columns:
                fig, ax = plt.subplots(1, 1, figsize=(6, 4))
                PartialDependenceDisplay.from_estimator(p, X_test, [f], grid_resolution=50, ax=ax)
                ax.set_title(f"PDP â€” {name} â€” {f}")
                st.pyplot(fig)

"""Optimasi/tuning via CV"""
# helper RMSE (kompatibel versi sklearn lama/baru)
def rmse_metric(y_true, y_pred):
  try:
    return mean_squared_error(y_true, y_pred, squared=False)
  except TypeError:
    return np.sqrt(mean_squared_error(y_true, y_pred))

# Rekap sebelum tuning
df_pre = df_res[['name','mae','rmse','r2']].copy()
df_pre.columns = ['model','mae_pre','rmse_pre','r2_pre']

## Tuning"""
cv = KFold(n_splits=5, shuffle=True, random_state=42)
scoring = "neg_mean_absolute_error"

cands = []

ridge_pipe = Pipeline([("preprocess", preprocess_linear),
  ("model", Ridge(random_state=42))])
cands.append(("Ridge", ridge_pipe, {"model__alpha":[0.1,1.0,10.0]}))

lasso_pipe = Pipeline([("preprocess", preprocess_linear),
  ("model", Lasso(random_state=42, max_iter=10000))])
cands.append(("Lasso", lasso_pipe, {"model__alpha":[0.001,0.01,0.1,1.0]}))

tree_pipe = Pipeline([("preprocess", preprocess_tree),
  ("model", DecisionTreeRegressor(random_state=42))])
cands.append(("Decision Tree", tree_pipe, {
  "model__max_depth":[3,5,8,None],
  "model__min_samples_leaf":[1,3,5]
}))

rf_pipe = Pipeline([("preprocess", preprocess_tree),
  ("model", RandomForestRegressor(random_state=42, n_jobs=-1))])
cands.append(("Random Forest", rf_pipe, {
    "model__n_estimators":[200,400],
    "model__max_depth":[None,8,12],
    "model__min_samples_leaf":[1,2,4]
}))

HAS_XGB = False
try:
  from xgboost import XGBRegressor
  HAS_XGB = True
  xgb_pipe = Pipeline([("preprocess", preprocess_tree),
    ("model", XGBRegressor(
      random_state=42, n_jobs=-1, tree_method="hist", eval_metric="mae"
    ))])
  cands.append(("XGBoost", xgb_pipe, {
    "model__n_estimators":[300,600],
    "model__max_depth":[3,6],
    "model__learning_rate":[0.05,0.1],
    "model__subsample":[0.8,1.0],
    "model__colsample_bytree":[0.8,1.0]
  }))
except Exception:
  pass

## GridSearchCV tiap model + Evaluasi Test"""

rows_post = []
best_models = {}
for name, pipe, grid in cands:
  gs = GridSearchCV(pipe, grid, cv=cv, scoring=scoring, n_jobs=-1, refit=True, verbose=0)
  gs.fit(X_train, y_train)
  best_models[name] = gs.best_estimator_

  y_pred = gs.predict(X_test)
  rows_post.append({
    "model": name,
    "mae_post":  mean_absolute_error(y_test, y_pred),
    "rmse_post": rmse_metric(y_test, y_pred),
    "r2_post":   r2_score(y_test, y_pred),
    "best_params": gs.best_params_
  })

df_post = pd.DataFrame(rows_post)

# Gabungan before vs after dan urutkan
cmp = (df_pre.merge(df_post, on="model", how="outer")
  .sort_values("mae_post")
  .reset_index(drop=True))
st.dataframe(cmp, use_container_width=True)

best_name = cmp.iloc[0]["model"]
st.write(f"\n>> Best by CV (MAE): {best_name}\nParams: {cmp.iloc[0]['best_params']}")
best_pipe = best_models[best_name]

"""Feature engineering"""

model_final = best_pipe
y_pred = model_final.predict(X_test)
if 'X_train_fe' in globals() and 'X_test_fe' in globals():
  try:
    best_pipe.fit(X_train_fe, y_train)
    _y_pred_fe = best_pipe.predict(X_test_fe)
    st.write("[info] Refit best_pipe dengan fitur rekayasa berhasil.")
    st.write("MAE (FE): ", mean_absolute_error(y_test, _y_pred_fe))
  except Exception as e:
    st.write("[warn] Refit dengan fitur rekayasa gagal, pakai fitur lama saja:", e)

## Tambah fitur waktu (ordinal & cyclical) + interaksi

### Fitur waktu dari Time_of_Day

def add_time_features(df: pd.DataFrame) -> pd.DataFrame:
  df = df.copy()
  if "Time_of_Day" in df.columns:
    # Ordinal
    order = {"Night": 0, "Morning": 1, "Afternoon": 2, "Evening": 3}
    df["Time_of_Day_ord"] = df["Time_of_Day"].map(order)

    # Cyclical (asumsi representatif jam: Night=2, Morning=9, Afternoon=15, Evening=20)
    hour_map = {"Night": 2, "Morning": 9, "Afternoon": 15, "Evening": 20}
    h = df["Time_of_Day"].map(hour_map).astype(float)
    h = h.fillna(np.nanmedian(h))
    df["tod_sin"] = np.sin(2 * np.pi * h / 24.0)
    df["tod_cos"] = np.cos(2 * np.pi * h / 24.0)
  return df

### Interaksi: Distance_km Ã— Traffic_Level; Preparation_Time_min Ã— Weather

def build_interaction_maker(X_train: pd.DataFrame):
  levels_traffic = X_train["Traffic_Level"].dropna().unique().tolist() if "Traffic_Level" in X_train.columns else []
  levels_weather = X_train["Weather"].dropna().unique().tolist() if "Weather" in X_train.columns else []

  def add_interactions(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # Distance_km Ã— 1{Traffic_Level = level}
    if "Distance_km" in df.columns and levels_traffic:
      for lv in levels_traffic:
        col = f"Dist_x_Traffic[{lv}]"
        df[col] = np.where(df.get("Traffic_Level") == lv, df["Distance_km"], 0.0)
    # Preparation_Time_min Ã— 1{Weather = level}
    if "Preparation_Time_min" in df.columns and levels_weather:
      for lv in levels_weather:
        col = f"Prep_x_Weather[{lv}]"
        df[col] = np.where(df.get("Weather") == lv, df["Preparation_Time_min"], 0.0)
    return df
  return add_interactions

# Apply ke TRAIN/TEST
X_train_fe = add_time_features(X_train)
X_test_fe  = add_time_features(X_test)

add_interactions = build_interaction_maker(X_train_fe)
X_train_fe = add_interactions(X_train_fe)
X_test_fe  = add_interactions(X_test_fe)

"""Update daftar kolom numerik & kategorikal untuk ColumnTransformer"""

num_cols = X_train_fe.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X_train_fe.select_dtypes(include=["object", "string"]).columns.tolist()

st.write("Numerik baru:", [c for c in ["Time_of_Day_ord","tod_sin","tod_cos"] if c in num_cols][:3], "...")
st.write("Contoh interaksi numerik:", [c for c in X_train_fe.columns if c.startswith(("Dist_x_Traffic","Prep_x_Weather"))][:4])

"""Final evaluation (setelah tuning)"""

# helper RMSE: kompatibel untuk sklearn lama/baru
def rmse_metric(y_true, y_pred):
  try:
    return mean_squared_error(y_true, y_pred, squared=False)
  except TypeError:
    return np.sqrt(mean_squared_error(y_true, y_pred))

## Model terbik hasil Tuning + Prediksi dalam Metrics"""

# Ambil model terbaik hasil tuning
if "best_pipe" in globals():
  final_model = best_pipe
elif "best_models" in globals() and "best_name" in globals():
  final_model = best_models[best_name]
else:
  raise RuntimeError("Model terbaik tidak ditemukan. Jalankan blok tuning dulu.")

# Prediksi
y_tr_pred = final_model.predict(X_train)
y_te_pred = final_model.predict(X_test)

# Metrics
metrics = {
  "split": ["Train","Test"],
  "MAE": [mean_absolute_error(y_train, y_tr_pred),
    mean_absolute_error(y_test,  y_te_pred)],
  "RMSE": [rmse_metric(y_train, y_tr_pred),
    rmse_metric(y_test,  y_te_pred)],
  "R2": [r2_score(y_train, y_tr_pred),
    r2_score(y_test,  y_te_pred)],
}
df_final_eval = pd.DataFrame(metrics)
st.write(" Final Evaluation (post-tuning)")
st.dataframe(df_final_eval.round(3), use_container_width=True)

# Visualisasi Diagnostik"""
st.subheader("Visualisasi Diagnostik (tooltip)")

# DIAGNOSTIK: Actual/Pred/Residual (aman dari NotFittedError) ===
import pandas as pd
import altair as alt
from sklearn.utils.validation import check_is_fitted
import streamlit as st

# 1) Ambil model dari kandidat yang mungkin kamu punya
def _resolve_model():
    cand = []
    # prioritas: yang ada di session_state dulu
    if 'model' in st.session_state and st.session_state['model'] is not None:
        cand.append(st.session_state['model'])
    # variabel umum yang sering dipakai setelah tuning
    if 'best' in globals(): cand.append(globals()['best'])
    if 'grid' in globals() and getattr(globals()['grid'], 'best_estimator_', None) is not None:
        cand.append(globals()['grid'].best_estimator_)
    if 'pipe' in globals(): cand.append(globals()['pipe'])
    for m in cand:
        if m is not None:
            return m
    return None

model = _resolve_model()
if model is None:
    st.error("Model tidak ditemukan (nggak ada 'model', 'best', 'grid.best_estimator_', atau 'pipe').")
    st.stop()

# 2) Pastikan sudah fitted; kalau belum dan ada data latih, langsung fit
def _ensure_fitted(m):
    try:
        check_is_fitted(m)
        return m
    except Exception:
        if 'X_train' in globals() and 'y_train' in globals():
            m.fit(X_train, y_train)
            st.session_state['model'] = m  # simpan biar gak hilang saat rerun
            return m
        else:
            st.error("Model belum dilatih dan data latih (X_train, y_train) tidak tersedia.")
            st.stop()

model = _ensure_fitted(model)

# 3) Prediksi & residu
y_pred = model.predict(X_test)
resid  = y_test - y_pred
diag = pd.DataFrame({"actual": y_test, "pred": y_pred, "resid": resid})

# 4) Plot
c1, c2, c3 = st.columns(3)

with c1:
    st.caption("Actual vs Predicted")
    st.altair_chart(
        alt.Chart(diag).mark_circle().encode(
            x="actual:Q", y="pred:Q",
            tooltip=[alt.Tooltip("actual:Q", format=".2f"),
                     alt.Tooltip("pred:Q", format=".2f"),
                     alt.Tooltip("resid:Q", format=".2f")]
        ),
        use_container_width=True
    )

with c2:
    st.caption("Residual Histogram")
    st.altair_chart(
        alt.Chart(diag)
        .transform_bin("bin_resid", field="resid", bin=alt.Bin(maxbins=30))
        .mark_bar()
        .encode(
            x=alt.X("bin_resid:Q", title="Residual"),
            y=alt.Y("count():Q", title="Count"),
            tooltip=[alt.Tooltip("bin_resid:Q", title="Residual bin")]
        ),
        use_container_width=True
    )

with c3:
    st.caption("Residual vs Predicted")
    rule = alt.Chart(pd.DataFrame({"y":[0]})).mark_rule().encode(y="y:Q")
    st.altair_chart(
        alt.Chart(diag).mark_circle().encode(
            x="pred:Q", y="resid:Q",
            tooltip=[alt.Tooltip("pred:Q", format=".2f"),
                     alt.Tooltip("resid:Q", format=".2f")]
        ) + rule,
        use_container_width=True
    )
    
"""Finalisasi"""

st.subheader("Final Evaluation")
def rmse_metric_final(y_true, y_pred):
    try: return mean_squared_error(y_true, y_pred, squared=False)
    except TypeError: return np.sqrt(mean_squared_error(y_true, y_pred))

y_tr_pred = pipe.predict(X_train)
y_te_pred = pipe.predict(X_test)

df_final_eval = pd.DataFrame({
    "split": ["Train","Test"],
    "MAE": [mean_absolute_error(y_train, y_tr_pred),
            mean_absolute_error(y_test,  y_te_pred)],
    "RMSE": [rmse_metric_final(y_train, y_tr_pred),
             rmse_metric_final(y_test,  y_te_pred)],
    "R2": [r2_score(y_train, y_tr_pred),
           r2_score(y_test,  y_te_pred)],
}).round(3)
st.dataframe(df_final_eval, use_container_width=True)

TOL_MIN = st.slider("Toleransi Â± menit (untuk % within)", 1, 15, 5, 1)
pct_within = (np.abs(y_test - y_te_pred) <= TOL_MIN).mean() * 100.0
donut_df = pd.DataFrame({"label":["Within","Outside"], "value":[pct_within, 100-pct_within]})
st.altair_chart(
    alt.Chart(donut_df).mark_arc(innerRadius=70)
    .encode(theta="value:Q", color="label:N",
            tooltip=["label", alt.Tooltip("value:Q", format=".1f")])
    .properties(width=300, height=300),
    use_container_width=False
)
st.write(f"**% within Â±{TOL_MIN} menit:** {pct_within:.1f}%")
